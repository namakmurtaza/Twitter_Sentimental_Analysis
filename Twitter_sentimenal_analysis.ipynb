{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPdUDZUhsmB2Ncl9YeJ1ufJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namakmurtaza/Twitter_Sentimental_Analysis/blob/main/Twitter_sentimenal_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xiMy16YdPYa"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from textblob import Word, TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_columns\",None)\n",
        "pd.set_option(\"display.width\", 200)\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)"
      ],
      "metadata": {
        "id": "ci5m7qCJdoHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"/content/twitter_training.csv\")\n",
        "df_valid = pd.read_csv(\"/content/twitter_validation.csv\")"
      ],
      "metadata": {
        "id": "WxqGoxdGd-fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "kctg59VpeWxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "id": "AsmgnWrEebCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns=['id','information','status','comment']\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "gRVtttEcefJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.columns=['id','information','status','comment']\n",
        "df_valid.head()"
      ],
      "metadata": {
        "id": "1Ibu6fl-ejt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "S9RVlXf1eq3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid.info()"
      ],
      "metadata": {
        "id": "-PlAOEpveufA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = df_train.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "4YwKmIw9exOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.dropna(subset=['comment'])"
      ],
      "metadata": {
        "id": "HIhehoNqez7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "eLcM8Fsse1PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"comment\"] = df_train[\"comment\"].str.lower()\n",
        "df_valid[\"comment\"] = df_valid[\"comment\"].str.lower()"
      ],
      "metadata": {
        "id": "aoxg9ZPGe87q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"comment\"] = df_train[\"comment\"].str.replace(r\"[@&]\\S+|&|http\\S+|www\\S+|[^\\w\\s]\", \"\", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "df_valid[\"comment\"] = df_valid[\"comment\"].str.replace(r\"[@&]\\S+|&|http\\S+|www\\S+|[^\\w\\s]\", \"\", regex=True).str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "df_train[\"comment\"] = df_train[\"comment\"].str.replace(r\"[@&]\\S+|&|http\\S+|www\\S+|\\d+|[^\\w\\s]\", \"\", regex=True) \\\n",
        "                                         .str.replace(r\"\\s+\", \" \", regex=True) \\\n",
        "                                         .str.strip()\n",
        "\n",
        "df_valid[\"comment\"] = df_valid[\"comment\"].str.replace(r\"[@&]\\S+|&|http\\S+|www\\S+|\\d+|[^\\w\\s]\", \"\", regex=True) \\\n",
        "                                         .str.replace(r\"\\s+\", \" \", regex=True) \\\n",
        "                                         .str.strip()"
      ],
      "metadata": {
        "id": "ytVGhRuyfCnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "sw = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "ot6qu9-nfDr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"comment\"] = df_train[\"comment\"].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))\n",
        "df_valid[\"comment\"] = df_valid[\"comment\"].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))"
      ],
      "metadata": {
        "id": "QSWqUC7hfHS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df_train = pd.Series(\" \".join(df_train[\"comment\"]).split()).value_counts()\n",
        "temp_df_valid = pd.Series(\" \".join(df_valid[\"comment\"]).split()).value_counts()"
      ],
      "metadata": {
        "id": "oKI5f8t7fKJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df_train"
      ],
      "metadata": {
        "id": "hpnFYs_9fOK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drop_train = temp_df_train[temp_df_train <= 1]\n",
        "drop_valid = temp_df_valid[temp_df_valid <= 1]"
      ],
      "metadata": {
        "id": "rb8706VmfRGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"comment\"] = df_train[\"comment\"].apply(lambda x: \" \".join(x for x in x.split() if x not in drop_train))\n",
        "df_valid[\"comment\"] = df_valid[\"comment\"].apply(lambda x: \" \".join(x for x in x.split() if x not in drop_valid))"
      ],
      "metadata": {
        "id": "RdcbBBsLfSNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head(10)"
      ],
      "metadata": {
        "id": "QFLLsUHkfWKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count information per category\n",
        "plot1=df_train.groupby(by=[\"information\",\"status\"]).count().reset_index()\n",
        "plot1.head()"
      ],
      "metadata": {
        "id": "47IxUz-6faSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Figure of comparison per branch\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.barplot(data=plot1,x=\"information\",y=\"id\",hue=\"status\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.xlabel(\"Brand\")\n",
        "plt.ylabel(\"Number of tweets\")\n",
        "plt.grid()\n",
        "plt.title(\"Distribution of tweets per Branch and Type\");"
      ],
      "metadata": {
        "id": "GVWSmS13fdPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment distribution\n",
        "sns.countplot(x='status', data=df_train, palette='viridis')\n",
        "plt.title(\"Distribution of Status\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xp5WpLzHfk93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud\n",
        "text = \" \".join(i for i in df_train.comment)\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title(\"Most Common Words in Tweets\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "As2KBb3gfqHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud positive\n",
        "# \"Positive\" filter\n",
        "positive_comments = df_train[df_train[\"status\"] == \"Positive\"][\"comment\"]\n",
        "\n",
        "text_positive = ''.join(i for i in positive_comments)\n",
        "wordcloud = WordCloud(background_color=\"black\").generate(text_positive)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title(\"Positive Common Words\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7dgDqtgtfvHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wordcloud negative\n",
        "# \"Negative\" filter\n",
        "negative_comments = df_train[df_train[\"status\"] == \"Negative\"][\"comment\"]\n",
        "\n",
        "text_positive = ''.join(i for i in negative_comments)\n",
        "wordcloud = WordCloud(background_color=\"black\").generate(text_positive)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.title(\"Negative Common Words\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "47kUDIyMf1hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "_dioBP4jf9eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these commands in separate cells\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "7UFXxMXSghlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Simple Split\n",
        "tokens_text = [str(word).split() for word in df_train[\"comment\"]]\n",
        "tokens_counter = [item for sublist in tokens_text for item in sublist]\n",
        "print(\"Number of tokens: \", len(set(tokens_counter)))\n",
        "\n",
        "# Method 2: Regex Tokenization\n",
        "import re\n",
        "tokens_text = [re.findall(r'\\w+', str(word)) for word in df_train[\"comment\"]]\n",
        "tokens_counter = [item for sublist in tokens_text for item in sublist]\n",
        "print(\"Number of tokens: \", len(set(tokens_counter)))"
      ],
      "metadata": {
        "id": "A8QNSi_WhAMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import re\n",
        "\n",
        "# Custom regex tokenization\n",
        "def regex_tokenize(text):\n",
        "    return re.findall(r'\\w+', str(text))\n",
        "\n",
        "# Convert text to numerical features\n",
        "vectorizer = CountVectorizer(tokenizer=regex_tokenize, ngram_range=(1, 2))\n",
        "X = vectorizer.fit_transform(df_train['comment'])\n",
        "y = df_train['status']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a simple classifier\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "I9Fob1xIhJrr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}